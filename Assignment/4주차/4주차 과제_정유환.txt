머신러닝의 목적: 비용함수(일반적으로 평균 제곱 오차)를 최소화하는 방식

-선형회귀 경우 2가지 방법 1. 정규방정식 혹은 특이값 분해
			2. 경사하강법

경사하강법 요약:1) 극대나 극소 찾는 방법(주로 도함수 이용)

2)학습률이 중요함(작으면 연산량^, 크면 목표치에 달성을 못할 수 있음)
2-1)특성스케일링 중요함.(타원보다 원 형태, 뭔느낌알?)

3.경사하강법 종류 [배치 경사하강법, 확률적 경사하강법, 미니배치 경사하강법)
3-1)배치 경사하강법: 에포크마다 한 번 그레디언트 계산(한 번에 움직임,but 메모리 문제, 연산 너무 많음)
3-2) 확률적 경사하강법: 그레디언트 계산 많음, 데이터 많은 세트 처리 가능, but 데이터마다 영향을 많이 받기 때문에 발산할 수도 있음
3-3) 미니배치 경사하강법: 두 경사하강법의 중간버전, but 극소에 수렴 가능성 있음

4)모델 규제
4-1) 릿지회귀: 비용함수 식 조작함으로써 파라미터 절댓값 낮게 조정
4-2) 라쏘회귀: 분산 줄어듦. 편향 늘게 하는 기법 중요치 않은 것들을 가중치를 0으로 만듦.
4-3) 엘라스틱 회귀: 두 회귀 섞어서 사용

5) 조기 종료: 과적합 문제 방지 위해 

6)로지스틱 회귀: 단일 회귀/

7)소프트맥스 회귀: 다중 회귀



단어) 배치 크기: 몇단위마다 다시 파라미터 갱신할건지
 그레디언트 벡터:경도란 벡터 미적분학에서 스칼라장의 최대의 증가율을 나타내는 벡터장을 뜻한다. 그레디언트 벡터랑 반대방향으로 가야 전역 최솟값으로 가까워짐.
학습스케줄: 훈련하면서 학습률 낮추는 기법
편향: 언더피팅 되었을때
분산: 오버피팅 되었을때 
(trade-off 관계)